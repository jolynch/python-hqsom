\documentclass[a4paper,10pt]{article}

\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{enumerate}
\usepackage{mdwlist}

%opening
\title{An Exploration of HQSOMS}
\author{Ted Hilk, Joseph Lynch}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
\section{Literature Review}
\section{Hypothesis, Plan, and Risks}
\section{System Design and Variables}
HQSOM based networks consist of three basic building blocks: the Self Organizing Map (SOM), the
Recursive Self Organizing Map (RSOM) and the SOM-RSOM pair. 
\subsection{SOM}
The basic SOM computional block is a fairly simple object that can either be shown data or asked to
classify data. During the training stage, input vectors are applied to the input and then an
update rule is applied over the entire map space that shifts map units towards the training data. 
The purpose of these training steps is to create clusters of map units that are spatially similar to
observed inputs.  Any given input can be mapped to its  
\subsection{RSOM}
\subsection{HQSOM}

\section{Implementation Process and Results}
\subsection{Implementation of SOM and RSOM Units}
Self-Organizing Maps were implemented via a python class with three main methods: a constructor,
an update method that takes a numpy array representing the input vector as well as the learning
parameters (gamma, sigma, etc...) and modifies the internal state of the SOM accordingly, and a
method to request the activation vector for a given input vector. Internally the SOM map is stored
as a $nxm$ numpy array where $n$ is the input vector size and $m$ is the size of the map space. 
During an update call, the Best Matching Map Unit (BMU) for any given input vector is determined
using a linear search for the minimum Euclidean distance and then all map units near to the BMU as
well as the BMU are shifted towards the input according to a Gaussian neighborhood function.  The
standard SOM update rule was used and is re-stated for convenience in equation ($\ref{eq:UPDATE}$).
 
\begin{equation} \label{eq:UPDATE}
 \bold{w_{i}}(t+1) = \bold{w_i}(t) + \gamma h_{ib}(t)(\bold{x}(t)-\bold{w_i}(t))
\end{equation}

A linear search was prefered due to the high dimensionality of the space <CITATION NEEDED>, and a
Gaussian neighborhood function was chosen for simplicity <MENTION MEXICAN HAT>.  The activation
method returns either a discrete or a continuous representation of the mean-squared error between
the input and any given map unit. The discrete activation vector is simply a $m$ dimension vector
with the BMU index for a given input vector set to 1 and all others set to 0. The continuous
activation vector is implemented using two different methodologies: a normal representation and a
regularized representation. Both methods consist of defining a $m$ dimension vector.  In the case
of the normal representation, equation ($\ref{eq:EUCMETH}$) is used and in the case of the
regularized representation, equation ($\ref{eq:MSEMETH}$) is used, where $a_i$ represents the
$i$th position in the activation vector, $\bold{w_b}$ is the BMU, $\bold{x}$ is the input vector,
$\bold{w_i}$ represents the $i$th map unit, and $MSE(\bold{x},\bold{y})$ indicates the mean squared
error between vectors $\bold{x}$ and $\bold{y}$.
\begin{equation} \label{eq:EUCMETH}
 a_i = \frac{1}{||\bold{w_i}-\bold{x}||}, \bold{a} = \frac{\bold{a}}{||\bold{a}||}
\end{equation}
\begin{equation} \label{eq:MSEMETH}
 a_i = \frac{MSE(\bold{w_b}, \bold{x})^3}{MSE(\bold{w_i},\bold{x})^3}
\end{equation}
The first method is fairly common in SOM implementations, but the second is intended to downplay the
results of non-matching units, which should theoretically increase orthagonality while preserving
noise tolerance.
\\
Recursive Self-Organizing Maps are simply a subclass of the SOM that use the modified RSOM update
rule as well as one additional variable: the recursive difference matrix.  The time decay parameter
is passed in at every update call.
\subsection{Basic Design of SOM-RSOM Pair}
Since both SOMs and RSOMs were implemented as Python objects, the SOM-RSOM pair simply consists of
a SOM object and a RSOM object with an update and activation method that takes in an input vector
$\bold{x}$, feeds it into the SOM to get a transformed activation vector $\bold{y}$ and finally
takes that $\bold{y}$ and feeds it into the RSOM to get the final output which is the BMU of the
RSOM.  The only difference between the two methods is that the update method is calling update
internally while the activation method is just checking activation vectors.  In the code this
SOM-RSOM pair is refered to as a Hierarchical Quilted Self-Organizing Map (HQSOM), even though
technically you need to build a network of SOM-RSOM pairs before it is an HQSOM.
\subsection{Replication of First Experiment}
The first experiment presented in the paper was a simple example of 3x3 images with 3
pixel horizontal and vertical lines that have been shifted to all possible positions.  This data
set is small enough to be enumerated, and simple enough in concept to use a single SOM-RSOM pair as
the HQSOM network.  The implementation is shown to be correct by three different tests: the first
tests performance on non-noisy data, the second tests performance on noisy data, and the last tests
performance on noisy data that has not been seen in training.  During tests, the HQSOM is exposed
to three blank images, followed by three line images, followed by three blank images where the
three line images alternate between the three horizontal and the three vertical images.  An example
test sequence is shown in $\ref{fig:3TestData}$.


\subsection{Replication of Second Experiment}
\subsection{Extension into Audio}
\subsection{Problems}
\section{Discussion and Conclusions}
\section{References}

\begin{thebibliography}{}
\bibitem{HQSOM} J. W. Miller and P. H. Lommel. \textsc{Biomimetic sensory abstraction using
hierarchical quilted self-organizing maps}. The Charles Stark Draper Laboratory, Inc.
555 Technology Square, Cambridge, MA 02139-3563, USA. 2006.
\end{thebibliography}

\end{document}
